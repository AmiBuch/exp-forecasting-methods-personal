{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM2izo5NCAzB7y1C/FNZX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiBuch/exp-forecasting-methods-personal/blob/main/scripts/self_attention_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dSBo7VqOyGq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention(query, key, value):\n",
        "  attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "  attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "  return torch.matmul(attention_weights, value)\n",
        "\n",
        "query = torch.tensor([[1,0.5]])\n",
        "key = torch.tensor([[1, 2], [0.5, 1]])\n",
        "value = torch.tensor([[0.1, 0.2], [0.3, 0.1]])\n",
        "new_representation = self_attention(query, key, value)\n",
        "print(new_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlkUP7UIY6Mf",
        "outputId": "0cddd016-aef6-461e-fda1-6084fff086e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1538, 0.1731]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import r2_score\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.encoding[:, :seq_len, :].to(x.device)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        attn_weights = self.softmax(torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model))\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        return out\n",
        "\n",
        "class TimeSeriesForecast(nn.Module):\n",
        "    def __init__(self, input_len, seq_len, d_model, forecast_len):\n",
        "        super(TimeSeriesForecast, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.num_chunks = input_len // seq_len\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
        "        self.self_attn = SelfAttention(d_model)\n",
        "        self.fc = nn.Linear(self.num_chunks * seq_len * d_model, forecast_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Reshape x to (batch_size * num_chunks, seq_len, d_model)\n",
        "        x = x.view(batch_size, self.num_chunks, self.seq_len, self.d_model)\n",
        "        x = x.view(batch_size * self.num_chunks, self.seq_len, self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.self_attn(x)\n",
        "        # Reshape x back to (batch_size, num_chunks, seq_len * d_model)\n",
        "        x = x.view(batch_size, self.num_chunks, self.seq_len * self.d_model)\n",
        "        # Flatten to (batch_size, num_chunks * seq_len * d_model)\n",
        "        x = x.view(batch_size, -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
        "air_passengers = pd.read_csv(url, index_col='Month', parse_dates=True)\n",
        "\n",
        "# Convert the 'Passengers' column to a numpy array\n",
        "raw_seq = air_passengers['Passengers'].values\n",
        "\n",
        "# Parameters\n",
        "input_len = 48\n",
        "forecast_len = 12\n",
        "d_model = 1\n",
        "seq_len = 8\n",
        "\n",
        "# Split the dataset\n",
        "train_seq = raw_seq[:132]\n",
        "test_seq = raw_seq[132 - input_len:144]  # Include 48 values before the last 12 for input\n",
        "\n",
        "# Create input-output pairs for training\n",
        "def create_sequences(data, input_len, forecast_len):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - input_len - forecast_len + 1):\n",
        "        input_seq = data[i:i+input_len]\n",
        "        target_seq = data[i+input_len:i+input_len+forecast_len]\n",
        "        inputs.append(input_seq)\n",
        "        targets.append(target_seq)\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "train_inputs, train_targets = create_sequences(train_seq, input_len, forecast_len)\n",
        "\n",
        "# Reshape inputs to (batch_size, input_len, d_model)\n",
        "train_inputs = train_inputs.reshape((train_inputs.shape[0], input_len, d_model))\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n",
        "train_targets = torch.tensor(train_targets, dtype=torch.float32)\n",
        "\n",
        "# Model instantiation\n",
        "model = TimeSeriesForecast(input_len=input_len, seq_len=seq_len, d_model=d_model, forecast_len=forecast_len)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 3000\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(train_inputs)\n",
        "    loss = criterion(output, train_targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Prepare the test input\n",
        "test_input = test_seq[:input_len].reshape(1, input_len, d_model)\n",
        "test_input = torch.tensor(test_input, dtype=torch.float32)\n",
        "\n",
        "# Predict the last 12 values\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_input)\n",
        "    print(\"Predicted values:\", test_output.numpy().flatten())\n",
        "    print(\"Actual values:\", test_seq[input_len:])\n",
        "    # Calculate MAE\n",
        "    mae = np.mean(np.abs(test_output.numpy().flatten() - test_seq[input_len:]))\n",
        "    print(f\"MAE: {mae}\")\n",
        "\n",
        "    # Calculate R^2\n",
        "    r2 = r2_score(test_seq[input_len:], test_output.numpy().flatten())\n",
        "    print(f\"R^2: {r2}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-ALPPItZXwa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1229ccc-ad74-4d81-e895-2a3c3a56c542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/3000], Loss: 39.7041\n",
            "Epoch [200/3000], Loss: 38.6019\n",
            "Epoch [300/3000], Loss: 37.2777\n",
            "Epoch [400/3000], Loss: 35.7736\n",
            "Epoch [500/3000], Loss: 34.1552\n",
            "Epoch [600/3000], Loss: 32.6248\n",
            "Epoch [700/3000], Loss: 31.1989\n",
            "Epoch [800/3000], Loss: 30.0285\n",
            "Epoch [900/3000], Loss: 29.1382\n",
            "Epoch [1000/3000], Loss: 28.4780\n",
            "Epoch [1100/3000], Loss: 28.0808\n",
            "Epoch [1200/3000], Loss: 27.8214\n",
            "Epoch [1300/3000], Loss: 27.6571\n",
            "Epoch [1400/3000], Loss: 27.5375\n",
            "Epoch [1500/3000], Loss: 27.4611\n",
            "Epoch [1600/3000], Loss: 27.3921\n",
            "Epoch [1700/3000], Loss: 27.3498\n",
            "Epoch [1800/3000], Loss: 27.2980\n",
            "Epoch [1900/3000], Loss: 27.2786\n",
            "Epoch [2000/3000], Loss: 27.2466\n",
            "Epoch [2100/3000], Loss: 27.2119\n",
            "Epoch [2200/3000], Loss: 27.1979\n",
            "Epoch [2300/3000], Loss: 27.1644\n",
            "Epoch [2400/3000], Loss: 27.1628\n",
            "Epoch [2500/3000], Loss: 27.1335\n",
            "Epoch [2600/3000], Loss: 27.1128\n",
            "Epoch [2700/3000], Loss: 27.0888\n",
            "Epoch [2800/3000], Loss: 27.0815\n",
            "Epoch [2900/3000], Loss: 27.0613\n",
            "Epoch [3000/3000], Loss: 27.0482\n",
            "Predicted values: [421.84094 439.26416 462.29602 490.57394 540.4382  551.4954  548.7676\n",
            " 558.0488  504.4385  514.4589  505.8159  497.22827]\n",
            "Actual values: [417 391 419 461 472 535 622 606 508 461 390 432]\n",
            "MAE: 47.51306915283203\n",
            "R^2: 0.42445178638137426\n"
          ]
        }
      ]
    }
  ]
}